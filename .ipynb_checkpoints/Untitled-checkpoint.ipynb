{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "# from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from sklearn import svm\n",
    "# import sklearn.model_selection\n",
    "# import sklearn.metrics\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "import math \n",
    "# import tensorflow as tf\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "import torch.utils.data as data\n",
    "from PIL import Image \n",
    "import pickle\n",
    "from PIL import Image as im\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.metrics import Metric\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import clear_output\n",
    "import argparse \n",
    "import pickle as pkl\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "### Network Debugging\n",
    "#########################################################################\n",
    "\n",
    "### Creating function for Gradient Visualisation \n",
    "def plot_grad_flow(result_directory,named_parameters,model_name): \n",
    "    \n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(result_directory, model_name + \"gradient_flow.png\" ))\n",
    "    \n",
    "### Get all the children layers \n",
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = []\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return model\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.extend(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.append(get_children(child))\n",
    "    return flatt_children\n",
    "    \n",
    "\n",
    "### Layer Activation in CNNs \n",
    "\n",
    "    \n",
    "def visualise_layer_activation(model,local_batch,result_directory):\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    layer_name = 'conv1'\n",
    "    model.module.conv1.register_forward_hook(get_activation(layer_name))\n",
    "    output = model(local_batch)\n",
    "    act = activation[layer_name].squeeze()\n",
    "    print(act.shape)\n",
    "    #plot subplots for different images\n",
    "    for i in range(act[0].shape[0]):\n",
    "        output = im.fromarray(np.uint8(np.moveaxis(act[0][i].cpu().detach().numpy(), 0, -1))).convert('RGB')\n",
    "        output.save(os.path.join(result_directory,str(i)+'.png'))\n",
    "        #\n",
    "\n",
    "### Visualising Conv Filters\n",
    "def visualise_conv_filters(model,result_directory):\n",
    "    kernels = model.conv1.weight.detach()\n",
    "    print(kernels.shape)\n",
    "    # fig, axarr = plt.subplots(kernels.size(0))\n",
    "    # for i in range(kernels.shape[0]):\n",
    "    #     plt.savefig()\n",
    "    # for idx in range(kernels.size(0)):\n",
    "    #     axarr[idx].imsave(kernels[idx].squeeze(),result_directory + \"1.png\")\n",
    "    #\n",
    "    \n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "#########################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data Preparation ############################################\n",
    "################################################################\n",
    "def load_images_from_folder(folder):\n",
    "    c=0\n",
    "    images = []\n",
    "    list_name=[]\n",
    "#     list_name1=[]\n",
    "    for filename in os.listdir(folder):\n",
    "        list_name.append(os.path.join(folder,filename))\n",
    "#         list_name1.append(filename)\n",
    "#     list_name1.sort()\n",
    "    list_name.sort()\n",
    "#     print(list_name1)\n",
    "    for filename in list_name:\n",
    "        img = cv2.imread(filename)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "#             print(c)\n",
    "#             c=c+1\n",
    "        # if c==8\n",
    "    return images\n",
    "\n",
    "def prepare_data(b_size=50):\n",
    "    train= load_images_from_folder('/scratch/harsh_cnn/SR_data/train/x')\n",
    "    train_input=np.asarray(train)\n",
    "    train_input=np.moveaxis(train_input,1,-1)\n",
    "    train_input=np.moveaxis(train_input,1,-1)\n",
    "    train_input = train_input.astype(np.float32)\n",
    "\n",
    "    train= load_images_from_folder('/scratch/harsh_cnn/SR_data/train/y')\n",
    "    train_target=np.asarray(train)\n",
    "    train_target=np.moveaxis(train_target,1,-1)\n",
    "    train_target=np.moveaxis(train_target,1,-1)\n",
    "    train_target = train_target.astype(np.float32)\n",
    "\n",
    "    test= load_images_from_folder('/scratch/harsh_cnn/SR_data/test/x')\n",
    "    test_input=np.asarray(test)\n",
    "    test_input=np.moveaxis(test_input,1,-1)\n",
    "    test_input=np.moveaxis(test_input,1,-1)\n",
    "    test_input = test_input.astype(np.float32)\n",
    "\n",
    "    test= load_images_from_folder('/scratch/harsh_cnn/SR_data/test/y')\n",
    "    test_target=np.asarray(test)\n",
    "    test_target=np.moveaxis(test_target,1,-1)\n",
    "    test_target=np.moveaxis(test_target,1,-1)\n",
    "    test_target = test_target.astype(np.float32)\n",
    "    data_train=[]\n",
    "    data_test=[]\n",
    "    \n",
    "    for input, target in zip(train_input, train_target):\n",
    "        data_train.append([input, target])\n",
    "\n",
    "    for input, target in zip(test_input, test_target):\n",
    "        data_test.append([input, target])\n",
    "\n",
    "    trainloader=torch.utils.data.DataLoader(dataset=data_train, batch_size=b_size, shuffle=True)\n",
    "    testloader=torch.utils.data.DataLoader(dataset=data_test, batch_size=b_size, shuffle=True)\n",
    "    return trainloader,testloader\n",
    "\n",
    "################################################################\n",
    "\n",
    "class DiscriminativeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DiscriminativeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=32, kernel_size=6, \n",
    "                stride=2, padding=2, bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32, out_channels=64, kernel_size=6,\n",
    "                stride=2, padding=2, bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64, out_channels=128, kernel_size=4,\n",
    "                stride=2, padding=1, bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=128, out_channels=256, kernel_size=4,\n",
    "                stride=4, padding=0, bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.BatchNorm2d(256)\n",
    "        )\n",
    "#         self.conv5 = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=256, out_channels=512, kernel_size=4,\n",
    "#                 stride=2, padding=1, bias=False\n",
    "#             ),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.BatchNorm2d(512)\n",
    "#         )\n",
    "#         self.conv6 = nn.Sequential(\n",
    "#             nn.Conv2d(\n",
    "#                 in_channels=512, out_channels=256, kernel_size=4,\n",
    "#                 stride=2, padding=1, bias=False\n",
    "#             ),\n",
    "#             nn.BatchNorm2d(256)\n",
    "#         )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=256, out_channels=128, kernel_size=4,\n",
    "                stride=4, padding=0, bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.Fc1 = nn.Sequential(\n",
    "            nn.Linear(128*4*4, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        self.Fc2 = nn.Sequential(\n",
    "            nn.Linear(512,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "#         x = self.conv5(x)\n",
    "#         x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        \n",
    "        # Flatten and apply sigmoid\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 128*4*4)\n",
    "#         print(x.shape)\n",
    "        x = self.Fc1(x)\n",
    "#         print(x.shape)\n",
    "        x = self.Fc2(x)\n",
    "#         print(x.shape)\n",
    "        # print(x)\n",
    "        return x\n",
    "    \n",
    "class SRSN(nn.Module):\n",
    "    def __init__(self, input_dim=3, dim=32, scale_factor=4):\n",
    "        super(SRSN, self).__init__()\n",
    "        self.up = torch.nn.Upsample(scale_factor=4, mode='bicubic')\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, 3, 1, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(64, 32, 1, 1, 0)\n",
    "        self.resnet = nn.Sequential(\n",
    "            ResnetBlock(dim, 3, 1, 1, bias=True),\n",
    "            ResnetBlock(dim, 3, 1, 1, bias=True),\n",
    "            ResnetBlock(dim, 3, 1, 1, bias=True),\n",
    "            ResnetBlock(dim, 3, 1, 1, bias=True),)\n",
    "        self.conv3=torch.nn.Conv2d(32, 16, 1, 1, 0)\n",
    "        self.conv4=torch.nn.Conv2d(16, 3, 1, 1, 0)\n",
    "\n",
    "    def forward(self, LR):\n",
    "        LR_feat = F.relu(self.conv1(self.up(LR)))\n",
    "        LR_feat = (F.relu(self.conv2(LR_feat)))\n",
    "        # print(LR_feat.shape)\n",
    "        LR_feat = self.resnet(LR_feat)\n",
    "        SR=F.relu(self.conv3(LR_feat))\n",
    "        SR =self.conv4(SR)\n",
    "        # print(SR.shape)\n",
    "        return SR\n",
    "\n",
    "class ResnetBlock(torch.nn.Module):\n",
    "    def __init__(self, num_filter, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride, padding, bias=bias)\n",
    "        self.conv2 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride, padding, bias=bias)\n",
    "\n",
    "        self.act1 = torch.nn.ReLU(inplace=True)\n",
    "        self.act2 = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.act1(x)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.act2(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = out + x\n",
    "\n",
    "        return out\n",
    "\n",
    "def train_discriminator(optimizer, real_data, fake_data,discriminator,b_loss):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = b_loss(prediction_real, Variable(torch.ones(real_data.size(0), 1)).to(device))\n",
    "    error_real.backward()\n",
    "\n",
    "    # 2. Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data.detach())\n",
    "    error_fake = b_loss(prediction_fake, Variable(torch.zeros(real_data.size(0), 1)).to(device))\n",
    "    error_fake.backward()\n",
    "    optimizer.step()\n",
    "    return error_real + error_fake, prediction_real, prediction_fake\n",
    "\n",
    "def train_generator(optimizer, fake_data,real_data,discriminator,b_loss,m_loss):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    lambda_ = 0.1\n",
    "    \n",
    "    ##Reconstruction loss\n",
    "    loss=m_loss(fake_data, real_data)\n",
    "    ## Adversarial Loss \n",
    "    prediction = discriminator(fake_data)\n",
    "    error = b_loss(prediction, Variable(torch.ones(real_data.size(0), 1)).to(device))\n",
    "    total_loss = lambda_*loss + error \n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss,error,total_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_network(debug,trainloader, testloader,num_epochs=200):\n",
    "\n",
    "    discriminator = DiscriminativeNet()\n",
    "    model=SRSN()\n",
    "    model = model.to(device)\n",
    "    discriminator=discriminator.to(device)\n",
    "\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    g_optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8)\n",
    "\n",
    "    Mse_loss = nn.MSELoss().to(device)\n",
    "    Bce_loss = nn.BCELoss().to(device)\n",
    "\n",
    "    train_d=[]\n",
    "    train_g = []\n",
    "    train_g_rec=[]\n",
    "    train_g_dis=[]\n",
    "    test=[]\n",
    "    \n",
    "    ## Parameters in Networks\n",
    "    print(\"Number of Parameters in Generator\")\n",
    "    count_parameters(model)\n",
    "    print(\"Number of Parameters in Discriminator\")\n",
    "    count_parameters(discriminator)\n",
    "    \n",
    "    results = \"/home/harsh.shukla/SRCNN/GAN_results\"\n",
    "    \n",
    "    if not os.path.exists(results):\n",
    "        os.makedirs(results)\n",
    "        \n",
    "    # Initialising Checkpointing directory \n",
    "    checkpoints = os.path.join(results,\"Checkpoints\")\n",
    "    if not os.path.exists(checkpoints):\n",
    "        os.makedirs(checkpoints)\n",
    "    checkpoint_file = os.path.join(checkpoints,\"check.pt\")  \n",
    "    \n",
    "    # Initialising directory for Network Debugging\n",
    "    net_debug = os.path.join(results,\"Debug\")\n",
    "    if not os.path.exists(net_debug):\n",
    "        os.makedirs(net_debug)\n",
    "    \n",
    "    model = nn.DataParallel(model, device_ids = device_ids)\n",
    "    discriminator = nn.DataParallel(discriminator, device_ids= device_ids)\n",
    "    # load model if exists\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(\"Loading from Previous Checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        g_optimizer.load_state_dict(checkpoint['g_state_dict'])\n",
    "        d_optimizer.load_state_dict(checkpoint['d_state_dict'])\n",
    "        model.train()\n",
    "        discriminator.train()\n",
    "    else:\n",
    "        print(\"No previous checkpoints exist, initialising network from start...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        training_loss_d=[]\n",
    "        training_rec_loss_g=[]\n",
    "        training_dis_loss_g=[]\n",
    "        training_loss_g = []\n",
    "        test_loss=[]\n",
    "\n",
    "        list_no=0\n",
    "        for input_,real_data in trainloader:\n",
    "            if torch.cuda.is_available():\n",
    "                input_ = input_.to(device)\n",
    "                real_data=real_data.to(device)\n",
    "\n",
    "            fake_data = model(input_)\n",
    "            d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer, real_data, fake_data,discriminator,Bce_loss)\n",
    "            g_rec_error,g_dis_error,g_error = train_generator(g_optimizer, fake_data, real_data,discriminator,Bce_loss,Mse_loss)\n",
    "            training_loss_d.append(d_error.item())\n",
    "            training_rec_loss_g.append(g_rec_error.item())\n",
    "            training_dis_loss_g.append(g_dis_error.item())\n",
    "            training_loss_g.append(g_error.item())\n",
    "        \n",
    "        #Plotting Gradient Flow for both the models\n",
    "        if(debug == True): \n",
    "            plot_grad_flow(net_debug,model.named_parameters(),\"generator\")\n",
    "            plot_grad_flow(net_debug,discriminator.named_parameters(),\"discriminator\")\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            for local_batch, local_labels in testloader:\n",
    "                local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "                output = model(local_batch).to(device)\n",
    "                local_labels.require_grad = False\n",
    "                test_loss.append(Mse_loss(output, local_labels).item())\n",
    "                \n",
    "        if debug == True:\n",
    "            label=im.fromarray(np.uint8(np.moveaxis(local_labels[0].cpu().detach().numpy(),0,-1))).convert('RGB')\n",
    "            output=im.fromarray(np.uint8(np.moveaxis(output[0].cpu().detach().numpy(),0,-1))).convert('RGB')\n",
    "            label.save(os.path.join(results,str(epoch) + 'test_target' + '.png'))\n",
    "            output.save(os.path.join(results,str(epoch) + 'test_output' + '.png'))\n",
    "        \n",
    "        #Calculating average loss per epoch\n",
    "        train_d.append(sum(training_loss_d)/len(training_loss_d))\n",
    "        train_g_rec.append(sum(training_rec_loss_g)/len(training_rec_loss_g))\n",
    "        train_g_dis.append(sum(training_dis_loss_g)/len(training_dis_loss_g))\n",
    "        train_g.append(sum(training_loss_g)/len(training_loss_g))\n",
    "        test.append(sum(test_loss)/len(test_loss))\n",
    "        ##Creating Checkpoints\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save({\n",
    "                'generator_state_dict': model.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'g_state_dict': g_optimizer.state_dict(),\n",
    "                'd_state_dict': d_optimizer.state_dict(),\n",
    "                }, checkpoint_file)\n",
    "            with open(os.path.join(results,\"Generator_loss.txt\"), 'wb') as f:\n",
    "                pkl.dump(train_g ,f)\n",
    "            with open(os.path.join(results,\"Discriminator_loss.txt\"), 'wb') as f:\n",
    "                pkl.dump(train_d ,f)\n",
    "            with open(os.path.join(results,\"Test_loss.txt\"), 'wb') as f:\n",
    "                pkl.dump(test,f )\n",
    "        print(\"Epoch :\",epoch )\n",
    "        print(\"Discriminator Loss :\",train_d[-1])\n",
    "        print(\"Generator Reconstruction Loss :\",train_g_rec[-1])\n",
    "        print(\"Generator Adversarial Loss :\",train_g_dis[-1])\n",
    "        print(\"Total Generator Loss:\",train_g[-1])\n",
    "\n",
    "        print(\"D(X) :\",d_pred_real.mean(), \"D(G(X)) :\",d_pred_fake.mean())\n",
    "        print(\"Test loss :\",test[-1])\n",
    "\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------\")\n",
    "    try:\n",
    "        file = open(os.path.join(results,\"GAN_train_loss.txt\"), 'w+')\n",
    "        try:\n",
    "            for i in range(len(test)):\n",
    "                file.write(str(train_d[i]) + \",\"  + str(train_g[i]) + \",\" + str(test[i]))\n",
    "                file.write('\\n')\n",
    "        finally:\n",
    "            file.close()\n",
    "    except IOError:\n",
    "        print(\"Unable to create loss file\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Training Completed\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-m','--debug', help=\"Mode of Execution here\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    grad_flow_flag = False\n",
    "\n",
    "\n",
    "    if args.debug == \"debug\": \n",
    "        print(\"Running in Debug Mode.....\")\n",
    "        grad_flow_flag = True\n",
    "\n",
    "    \n",
    "    trainloader, testloader = prepare_data(b_size=40)\n",
    "    print(\"Initialised Data Loaders\")\n",
    "    train_network(grad_flow_flag,trainloader, testloader,num_epochs=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "# from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from sklearn import svm\n",
    "# import sklearn.model_selection\n",
    "# import sklearn.metrics\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "import math \n",
    "# import tensorflow as tf\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "import torch.utils.data as data\n",
    "from PIL import Image \n",
    "import pickle\n",
    "from PIL import Image as im\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.metrics import Metric\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import clear_output\n",
    "import argparse \n",
    "import pickle as pkl\n",
    "x=Variable(torch.ones(real_data.size(0), 1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
